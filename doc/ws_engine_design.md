# 高性能撮合引擎 WebSocket 模块设计文档（支持千万级并发订阅）

---

## 目录

- [一、频道化设计：支持币种分房间](#一频道化设计支持币种分房间)
- [二、SubmitOrder 增强支持：支持传入交易对](#二submitorder-增强支持支持传入交易对)
- [三、千万级并发订阅能力优化](#三千万级并发订阅能力优化)
- [四、总结：最佳实践与技术选型建议](#四总结最佳实践与技术选型建议)

---

## 一、频道化设计：支持币种分房间

### 背景与目标
当前所有交易对的撮合和订单广播复用同一条 WebSocket 连接和频道，导致客户端需要自行过滤、服务端无法做精细控制。为了解决这个问题，引入频道（Channel）机制，将不同交易对划分到不同频道中。

### 设计目标
- 每个交易对（如 BTC-USDT）视为一个频道。
- 客户端可选择性订阅一个或多个频道。
- 频道内广播仅发送与该交易对相关的撮合消息、订单簿变更、成交数据。

### 接口定义

#### 客户端订阅消息格式
```json
{
  "action": "subscribe",
  "channel": "BTC-USDT"
}
```

#### 服务端返回
```json
{
  "type": "subscription_ack",
  "channel": "BTC-USDT"
}
```

#### 广播示例
```json
{
  "channel": "BTC-USDT",
  "type": "depth_update",
  "data": { "xx":"yy"}
}
```

### 服务端实现建议
- 基于 CloudWeGo Netpoll 提供每个连接的“订阅组”概念。
- 使用基于 `map[channel][]conn` 的结构维护频道订阅者。
- 引入 `shard` 分片机制避免热点锁争用。
- 频道分片建议按 hash(channel) % shard_num 分布，提升扩展性。

---

## 二、SubmitOrder 增强支持：支持传入交易对

### 背景
目前 SubmitOrder 请求结构中未明确指定交易对，在多币种撮合中不适应。

### 修改目标
支持多币种撮合场景，每个订单明确指定交易对，并将撮合逻辑分发至正确的撮合线程。

### 修改内容

#### 原始结构
```json
{
  "order_id": "abc123",
  "side": "buy",
  "price": "30000",
  "quantity": "0.5"
}
```

#### 增强结构
```json
{
  "order_id": "abc123",
  "symbol": "BTC-USDT",
  "side": "buy",
  "price": "30000",
  "quantity": "0.5"
}
```

### 服务端逻辑变更
- 撮合服务根据 `symbol` 分派到对应的撮合引擎（支持多线程或多进程分区）。
- `symbol` 用于校验订单合法性及查询对应订单簿状态。
- 日志与指标按交易对维度聚合。

---

## 三、千万级并发订阅能力优化

### 背景
当前基于 WebSocket + goroutine per conn 的模型，难以承载千万级并发连接，需从 I/O 模型与事件处理体系全面优化。

### 技术选型
- 网络层：使用 CloudWeGo Netpoll 替代标准 net/http，采用非阻塞多路复用。
- 消息推送：使用共享缓冲区 + 零拷贝（ring buffer / mmap）方式推送。
- 连接管理：事件循环模型 + 连接复用，单节点多 Reactor 协同。
- 数据结构：基于 SkipList / TreeMap 管理连接与频道绑定关系，结合分片（shard）机制。
- 分布式扩展：支持多节点水平扩展，频道分布式路由。

### 优化方向

#### 1. I/O 模型改造
- 使用 Netpoll 或 Mio 级别的非阻塞多路复用。
- 每个 Reactor 管理 10 万连接，多个 Reactor 协同工作。
- 连接对象池化，减少频繁创建销毁。

#### 2. 消息发布优化
- 每个频道维护共享消息缓冲区，推送时直接写入所有订阅连接。
- 零拷贝推送，减少内存和 CPU 消耗。
- 可选：使用 `fanout` 风格 Topic Broker（如 nanomsg 设计）。

#### 3. 数据结构优化
- 频道订阅关系采用分片 map，提升并发写入和查找效率。
- 连接与频道绑定关系建议用高效结构（如 SkipList/TreeMap），提升查找和推送效率。

#### 4. 压测指标
- 目标 TPS：千万级 order push / s。
- 延迟目标：P99 ≤ 5ms（连接数量 > 1000 万）。
- 资源消耗：单节点 ≤ 32GB RAM，CPU 使用率 ≤ 70%。
- 支持多节点分布式部署，单节点故障自动迁移。

---

## 四、总结：最佳实践与技术选型建议

| 模块          | 技术选型              | 理由与说明                         |
|---------------|-----------------------|------------------------------------|
| 网络通信      | CloudWeGo Netpoll     | 支持千万级高并发连接               |
| 消息协议      | WebSocket JSON        | 跨平台、易于调试、兼容性强         |
| 数据通道      | 分频道广播（Pub/Sub） | 降低客户端解析成本，提升服务端扩展性 |
| 撮合引擎      | 分 symbol 撮合线程池    | 提高并发撮合效率                   |
| 状态维护      | 基于 symbol 的缓存管理  | 精细化资源控制                     |
| 分布式扩展    | 多节点分片路由        | 支持横向扩展与高可用               |

---

> 本设计文档面向千万级并发场景，建议优先采用事件驱动、分片、零拷贝等高性能架构，结合分布式部署实现弹性扩展。
